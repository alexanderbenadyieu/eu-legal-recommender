# EU Legal Document Recommender System

A sophisticated recommender system designed to match EU legal documents with user interests based on semantic similarity, document features, and user preferences.

## Architecture Overview

### Core Components

1. **Text Embedding (embeddings.py)**
   - Uses Legal-BERT (nlpaueb/legal-bert-small-uncased) for domain-specific semantic text understanding
   - 512-dimensional embeddings optimized for legal text
   - Features:
     - Efficient batch processing
     - Embedding caching
     - GPU support when available

2. **Feature Processing (features.py)**
   - Handles categorical document attributes
   - Implements:
     - One-hot encoding for categorical features
     - Feature persistence and loading
     - Dynamic feature space adaptation

3. **Similarity Computation**
   - Hybrid similarity calculation combining:
     - Text embedding similarity (80% weight)
     - Categorical feature similarity (20% weight)
   - Uses Pinecone vector database for efficient similarity search
   - Supports metadata filtering and hybrid search

4. **Recommender System (pinecone_recommender.py)**
   - Leverages Pinecone vector database for scalable similarity search
   - Provides configurable ranking parameters
   - Supports hybrid search combining embedding similarity with categorical features
   - Includes metadata filtering capabilities

### Input Features

1. **Text Features**
   - Document summaries (generated by our summarization pipeline)
   - Keywords (extracted using KeyBERT)
   - User interest descriptions

2. **Categorical Features**
   - Document Type (regulation, directive, decision)
   - Subject Matter
   - Legal Basis
   - Geographic Scope
   - Temporal Validity

## Installation

```bash
# Create and activate virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\\Scripts\\activate

# Install dependencies
pip install -r requirements.txt
```

## Usage

### Basic Usage

```python
from recommender.src.models.embeddings import BERTEmbedder
from recommender.src.models.features import FeatureProcessor
from recommender.src.models.pinecone_recommender import PineconeRecommender

# Initialize components
embedder = BERTEmbedder(model_name='nlpaueb/legal-bert-small-uncased')
feature_processor = FeatureProcessor({
    'type': ['regulation', 'directive', 'decision', 'other'],
    'subject': ['environment', 'transport', 'energy', 'finance', 'other'],
    'scope': ['EU-wide', 'member_states', 'third_countries', 'other'],
    'legal_basis': ['TFEU_114', 'TFEU_192', 'TFEU_194', 'other']
})

# Create Pinecone recommender
recommender = PineconeRecommender(
    api_key='your_pinecone_api_key',
    index_name='eu-legal-documents-legal-bert',
    embedder_model='nlpaueb/legal-bert-small-uncased',
    feature_processor=feature_processor,
    text_weight=0.8,
    categorical_weight=0.2
)

# Get recommendations
recommendations = recommender.get_recommendations(
    query_text='Environmental regulations for reducing greenhouse gas emissions',
    query_keywords=['climate', 'emissions', 'environment'],
    query_features={
        'type': 'regulation',
        'subject': 'environment',
        'scope': 'EU-wide',
        'legal_basis': 'TFEU_192'
    },
    top_k=5
)

# Display results
for i, rec in enumerate(recommendations):
    print(f"{i+1}. Document ID: {rec['id']} (Score: {rec['score']:.4f})")
    print(f"   Text Similarity: {rec['text_score']:.4f}")
    if rec.get('categorical_score'):
        print(f"   Categorical Similarity: {rec['categorical_score']:.4f}")
```

### Advanced Configuration

```python
# Enable GPU acceleration
embedder = BERTEmbedder(
    model_name='nlpaueb/legal-bert-small-uncased',
    device='cuda'
)

# Configure Pinecone with metadata filtering
recommender = PineconeRecommender(
    api_key='your_pinecone_api_key',
    index_name='eu-legal-documents-legal-bert',
    environment='gcp-starter',  # Or your specific Pinecone environment
    embedder_model='nlpaueb/legal-bert-small-uncased',
    feature_processor=feature_processor,
    text_weight=0.8,
    categorical_weight=0.2
)

# Use metadata filtering for more targeted results
filter_dict = {
    "metadata": {
        "type": "regulation",
        "year": {"$gte": 2015}  # Documents from 2015 or later
    }
}
```

## Performance Optimization

1. **Batch Processing**
   - Documents are processed in batches
   - Configurable batch size for memory management
   - Parallel processing where applicable

2. **Caching System**
   - Embeddings cache
   - Feature encoding cache
   - FAISS index persistence

3. **GPU Acceleration**
   - BERT model can run on GPU
   - FAISS GPU support available
   - Batch size optimization for GPU memory

## Evaluation Metrics

1. **Relevance Metrics**
   - Mean Reciprocal Rank (MRR)
   - Normalized Discounted Cumulative Gain (NDCG)
   - Precision@k
   - Recall@k

2. **Performance Metrics**
   - Processing time per document
   - Query response time
   - Memory usage
   - GPU utilization (if applicable)

## Future Enhancements

1. **Model Improvements**
   - Fine-tune BERT model on legal documents
   - Implement attention mechanisms for keyword weighting
   - Add support for multi-lingual embeddings

2. **Feature Engineering**
   - Add temporal relevance scoring
   - Implement citation-based features
   - Add document complexity metrics

3. **System Optimization**
   - Implement distributed processing
   - Add real-time update capability
   - Optimize memory usage for large document sets

4. **User Interface**
   - Web-based interface for queries
   - Result visualization
   - User feedback integration

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the LICENSE file for details.

## Acknowledgments

- Built using the Sentence-Transformers library
- FAISS library for efficient similarity search
- scikit-learn for feature processing
