{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottling Company Optimization Results Analysis\n",
    "\n",
    "This notebook analyzes the weight optimization results for the bottling company client and explores why all configurations are returning zero scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Optimization Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the optimization results\n",
    "with open('bottling_optimization_results.json', 'r') as f:\n",
    "    bottling_results = json.load(f)\n",
    "\n",
    "# Print the structure to understand the data\n",
    "print(\"Keys in the results file:\")\n",
    "print(list(bottling_results.keys()))\n",
    "\n",
    "client_key = 'bottling_company_client'\n",
    "print(f\"\\nProfile configurations for {client_key}:\")\n",
    "print(list(bottling_results[client_key].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Test Data to Understand the Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the test data to examine ground truth documents\n",
    "with open('test_data.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# Examine the bottling company test data\n",
    "if client_key in test_data:\n",
    "    print(f\"\\nTest data available for {client_key}\")\n",
    "    \n",
    "    # Check if relevant_docs exists\n",
    "    if 'relevant_docs' in test_data[client_key]:\n",
    "        print(f\"Number of ground truth documents: {len(test_data[client_key]['relevant_docs'])}\")\n",
    "        print(\"Ground truth documents:\")\n",
    "        for doc in test_data[client_key]['relevant_docs']:\n",
    "            print(f\"  - {doc}\")\n",
    "    else:\n",
    "        print(\"No relevant_docs field found in test data\")\n",
    "        print(\"Available keys in test data:\")\n",
    "        print(list(test_data[client_key].keys()))\n",
    "else:\n",
    "    print(f\"No test data found for {client_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Profile Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load the bottling company profile to understand what's being used for recommendations\n",
    "try:\n",
    "    with open('../profiles/bottling_company_client.json', 'r') as f:\n",
    "        profile = json.load(f)\n",
    "    \n",
    "    print(\"Profile components:\")\n",
    "    for component, data in profile.items():\n",
    "        if component == 'historical_documents':\n",
    "            print(f\"\\n{component}: {len(data)} documents\")\n",
    "            for doc in data[:5]:  # Show first 5 documents\n",
    "                print(f\"  - {doc}\")\n",
    "            if len(data) > 5:\n",
    "                print(f\"  - ... and {len(data)-5} more\")\n",
    "        elif component == 'expert_profile':\n",
    "            print(f\"\\n{component}: {len(data.get('description', '').split())} words\")\n",
    "        elif component == 'categorical_preferences':\n",
    "            print(f\"\\n{component}:\")\n",
    "            for category, prefs in data.items():\n",
    "                print(f\"  - {category}: {len(prefs)} preferences\")\n",
    "        elif component == 'component_weights':\n",
    "            print(f\"\\n{component}:\")\n",
    "            for weight_name, weight_value in data.items():\n",
    "                print(f\"  - {weight_name}: {weight_value}\")\n",
    "        else:\n",
    "            print(f\"\\n{component}: {type(data)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Profile file not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Ground Truth with Historical Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check if ground truth documents overlap with historical documents\n",
    "try:\n",
    "    historical_docs = set(profile.get('historical_documents', []))\n",
    "    ground_truth_docs = set(test_data[client_key].get('relevant_docs', []))\n",
    "    \n",
    "    print(f\"Number of historical documents: {len(historical_docs)}\")\n",
    "    print(f\"Number of ground truth documents: {len(ground_truth_docs)}\")\n",
    "    \n",
    "    # Check overlap\n",
    "    overlap = historical_docs.intersection(ground_truth_docs)\n",
    "    print(f\"\\nNumber of overlapping documents: {len(overlap)}\")\n",
    "    if overlap:\n",
    "        print(\"Overlapping documents:\")\n",
    "        for doc in overlap:\n",
    "            print(f\"  - {doc}\")\n",
    "    \n",
    "    # Documents in ground truth but not in historical\n",
    "    missing = ground_truth_docs - historical_docs\n",
    "    print(f\"\\nDocuments in ground truth but not in historical: {len(missing)}\")\n",
    "    if missing:\n",
    "        print(\"Missing documents:\")\n",
    "        for doc in missing:\n",
    "            print(f\"  - {doc}\")\n",
    "except (KeyError, NameError):\n",
    "    print(\"Could not compare documents - data missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze the Nature of Zero Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Extract all weights and scores to understand patterns\n",
    "def extract_weights_results(results, profile_type, metric_key):\n",
    "    all_configs = []\n",
    "    \n",
    "    # Extract similarity weights\n",
    "    if 'similarity' in results[profile_type]:\n",
    "        for result in results[profile_type]['similarity'][metric_key]['all_results']:\n",
    "            config = {\n",
    "                'config_type': 'similarity',\n",
    "                'score': result['score']\n",
    "            }\n",
    "            config.update(result['weights'])\n",
    "            all_configs.append(config)\n",
    "    \n",
    "    # Extract personalization weights\n",
    "    if 'personalization' in results[profile_type]:\n",
    "        for result in results[profile_type]['personalization'][metric_key]['all_results']:\n",
    "            config = {\n",
    "                'config_type': 'personalization',\n",
    "                'score': result['score']\n",
    "            }\n",
    "            config.update(result['weights'])\n",
    "            all_configs.append(config)\n",
    "            \n",
    "    # Extract embedding weights\n",
    "    if 'embedding' in results[profile_type]:\n",
    "        for result in results[profile_type]['embedding'][metric_key]['all_results']:\n",
    "            config = {\n",
    "                'config_type': 'embedding',\n",
    "                'score': result['score']\n",
    "            }\n",
    "            config.update(result['weights'])\n",
    "            all_configs.append(config)\n",
    "    \n",
    "    return pd.DataFrame(all_configs)\n",
    "\n",
    "# Extract all weight configurations\n",
    "precision_df = extract_weights_results(bottling_results, client_key, 'precision@10')\n",
    "ndcg_df = extract_weights_results(bottling_results, client_key, 'ndcg@10')\n",
    "\n",
    "print(f\"Total weight configurations evaluated for precision: {len(precision_df)}\")\n",
    "print(f\"Total weight configurations evaluated for NDCG: {len(ndcg_df)}\")\n",
    "print(f\"\\nUnique precision scores: {precision_df['score'].unique()}\")\n",
    "print(f\"Unique NDCG scores: {ndcg_df['score'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Recommended Next Steps\n",
    "\n",
    "Based on the analysis above, here are some recommended next steps:\n",
    "\n",
    "1. **Adjust K value**: Try using smaller k values (e.g., k=3 or k=5) to increase the granularity of the evaluation metrics.\n",
    "\n",
    "2. **Review ground truth data**: Verify that the ground truth documents in test_data.json are correct and relevant to the bottling company profile.\n",
    "\n",
    "3. **Check for gaps between test data and historical documents**: If the test data documents are completely different from the historical documents, this could explain why we're seeing zero scores.\n",
    "\n",
    "4. **Test with alternative metrics**: Consider using Mean Reciprocal Rank (MRR) or evaluating with a larger k (e.g., k=20 or k=30) to potentially capture more relevant documents.\n",
    "\n",
    "5. **Examine the expert profile**: Review the bottling company expert profile to ensure it accurately reflects the company's interests and is specific enough to recommend relevant documents.\n",
    "\n",
    "6. **Check for document availability**: Verify that the ground truth documents are actually available in the index being searched by the recommender system.\n",
    "\n",
    "7. **Consider cross-validation**: If possible, use cross-validation techniques to ensure the evaluation is robust.\n",
    "\n",
    "8. **Expand test data**: Consider adding more relevant documents to the ground truth set to create a larger target for the recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Implementation of K Value Adjustment\n",
    "\n",
    "To quickly test whether smaller k values might yield better results, here's code to simulate what precision and recall would look like with different values of k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def simulate_precision_at_k(num_relevant_docs, relevant_positions, k_values):\n",
    "    \"\"\"Simulate precision@k for different k values\n",
    "    \n",
    "    Args:\n",
    "        num_relevant_docs: Total number of relevant documents\n",
    "        relevant_positions: List of positions (1-indexed) where relevant docs appear\n",
    "        k_values: List of k values to evaluate\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for k in k_values:\n",
    "        # Count relevant docs in top k\n",
    "        relevant_in_top_k = sum(1 for pos in relevant_positions if pos <= k)\n",
    "        precision = relevant_in_top_k / k if k > 0 else 0\n",
    "        recall = relevant_in_top_k / num_relevant_docs if num_relevant_docs > 0 else 0\n",
    "        results[k] = {'precision': precision, 'recall': recall}\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Let's assume we have 10 relevant documents and one of them appears at position 1\n",
    "# This is a hypothetical scenario to illustrate the impact of k\n",
    "relevant_positions = [1, 15, 23, 42, 50, 67, 85, 99, 110, 120]\n",
    "k_values = [1, 3, 5, 10, 20, 50, 100]\n",
    "\n",
    "results = simulate_precision_at_k(10, relevant_positions, k_values)\n",
    "\n",
    "# Create dataframe for visualization\n",
    "results_df = pd.DataFrame({\n",
    "    'k': list(results.keys()),\n",
    "    'precision': [result['precision'] for result in results.values()],\n",
    "    'recall': [result['recall'] for result in results.values()]\n",
    "})\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "results_df.plot(x='k', y='precision', marker='o', ax=ax, label='Precision@k')\n",
    "results_df.plot(x='k', y='recall', marker='s', ax=ax, label='Recall@k')\n",
    "ax.set_title('Impact of k Value on Precision and Recall')\n",
    "ax.set_xlabel('k Value')\n",
    "ax.set_ylabel('Score')\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display the numerical results\n",
    "print(\"Precision and Recall at different k values:\")\n",
    "print(results_df.round(3).to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
