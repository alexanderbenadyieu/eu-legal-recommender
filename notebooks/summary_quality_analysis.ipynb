{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EU Legal Document Summary Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /Users/alexanderbenady/DataThesis/eu-legal-recommender/venv/lib/python3.10/site-packages (from rouge) (1.17.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/alexanderbenady/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alexanderbenady/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database\n",
    "DB_PATH = \"../scraper/data/eurlex.db\"\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "- responsible_bodies\n",
      "- forms\n",
      "- documents\n",
      "- authors\n",
      "- document_authors\n",
      "- eurovoc_descriptors\n",
      "- document_eurovoc_descriptors\n",
      "- subject_matters\n",
      "- document_subject_matters\n",
      "- directory_codes\n",
      "- document_directory_codes\n",
      "- sqlite_sequence\n",
      "- document_sections\n",
      "- document_keywords\n"
     ]
    }
   ],
   "source": [
    "# List all tables in the database\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = cursor.fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(f\"- {table[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns in the documents table:\n",
      "- document_id (INTEGER)\n",
      "- celex_number (TEXT)\n",
      "- title (TEXT)\n",
      "- identifier (TEXT)\n",
      "- eli_uri (TEXT)\n",
      "- html_url (TEXT)\n",
      "- pdf_url (TEXT)\n",
      "- responsible_body_id (INTEGER)\n",
      "- form_id (INTEGER)\n",
      "- date_of_document (DATE)\n",
      "- date_of_effect (DATE)\n",
      "- date_of_end_validity (DATE)\n",
      "- content (TEXT)\n",
      "- content_html (TEXT)\n",
      "- summary (TEXT)\n",
      "- summary_word_count (INTEGER)\n",
      "- total_words (INTEGER)\n",
      "- compression_ratio (REAL)\n",
      "- tier (INTEGER)\n"
     ]
    }
   ],
   "source": [
    "# Examine schema of documents table\n",
    "cursor.execute(\"PRAGMA table_info(documents);\")\n",
    "columns = cursor.fetchall()\n",
    "print(\"\\nColumns in the documents table:\")\n",
    "for col in columns:\n",
    "    print(f\"- {col[1]} ({col[2]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Documents from Each Tier\n",
    "\n",
    "We'll retrieve documents from each tier, randomly sampling 10% of them for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define word count thresholds for tiers according to the project methodology\n",
    "def get_tier_from_word_count(word_count):\n",
    "    if word_count <= 600:\n",
    "        return 1\n",
    "    elif word_count <= 2500:\n",
    "        return 2\n",
    "    elif word_count <= 20000:\n",
    "        return 3\n",
    "    else:\n",
    "        return 4\n",
    "\n",
    "# Query to analyze documents by word count\n",
    "cursor.execute(\"\"\"\n",
    "    SELECT \n",
    "        content,\n",
    "        total_words,\n",
    "        tier\n",
    "    FROM documents \n",
    "    WHERE summary IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "# Process results\n",
    "word_count_tiers = {1: 0, 2: 0, 3: 0, 4: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Document counts by word count tier:\")\n",
    "for tier, count in word_count_tiers.items():\n",
    "    if tier == 1:\n",
    "        print(f\"Tier 1 (0-600 words): {count} documents\")\n",
    "    elif tier == 2:\n",
    "        print(f\"Tier 2 (601-2,500 words): {count} documents\")\n",
    "    elif tier == 3:\n",
    "        print(f\"Tier 3 (2,501-20,000 words): {count} documents\")\n",
    "    else:\n",
    "        print(f\"Tier 4 (20,000+ words): {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, check how many tiers are in the database and how many documents are in each tier\n",
    "cursor.execute(\"SELECT tier, COUNT(*) FROM documents GROUP BY tier ORDER BY tier;\")\n",
    "tier_counts = cursor.fetchall()\n",
    "\n",
    "print(\"Document counts by tier:\")\n",
    "for tier, count in tier_counts:\n",
    "    print(f\"Tier {tier}: {count} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to sample documents from a specific tier\n",
    "def sample_documents_from_tier(tier, sample_pct=0.1):\n",
    "    # Get total count for this tier\n",
    "    cursor.execute(\"SELECT COUNT(*) FROM documents WHERE tier = ? AND summary IS NOT NULL\", (tier,))\n",
    "    total_count = cursor.fetchone()[0]\n",
    "    \n",
    "    # Calculate sample size (10%)\n",
    "    sample_size = max(int(total_count * sample_pct), 1)  # At least 1 document\n",
    "    \n",
    "    # Get all document IDs for this tier\n",
    "    cursor.execute(\"SELECT celex_number FROM documents WHERE tier = ? AND summary IS NOT NULL\", (tier,))\n",
    "    all_ids = [row[0] for row in cursor.fetchall()]\n",
    "    \n",
    "    # Randomly sample document IDs\n",
    "    sampled_ids = random.sample(all_ids, min(sample_size, len(all_ids)))\n",
    "    \n",
    "    # Fetch the complete documents for the sampled IDs\n",
    "    placeholders = ', '.join(['?'] * len(sampled_ids))\n",
    "    query = f\"SELECT celex_number, title, content, summary, tier FROM documents WHERE celex_number IN ({placeholders})\"\n",
    "    cursor.execute(query, sampled_ids)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    columns = ['celex_number', 'title', 'content', 'summary','tier']\n",
    "    df = pd.DataFrame(cursor.fetchall(), columns=columns)\n",
    "    \n",
    "    print(f\"Sampled {len(df)} documents from Tier {tier} (out of {total_count})\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents from each tier\n",
    "all_samples = []\n",
    "for tier, _ in tier_counts:\n",
    "    tier_samples = sample_documents_from_tier(tier)\n",
    "    all_samples.append(tier_samples)\n",
    "\n",
    "# Combine all samples into one DataFrame\n",
    "sampled_docs = pd.concat(all_samples, ignore_index=True)\n",
    "sampled_docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:keyword_extractor:Initializing KeyBERT with model: distilbert-base-nli-mean-tokens\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: distilbert-base-nli-mean-tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting keywords for 416 documents...\n",
      "Processing document 1/416...\n",
      "Processing document 11/416...\n",
      "Processing document 21/416...\n",
      "Processing document 31/416...\n",
      "Processing document 41/416...\n",
      "Processing document 51/416...\n",
      "Processing document 61/416...\n",
      "Processing document 71/416...\n",
      "Processing document 81/416...\n",
      "Processing document 91/416...\n",
      "Processing document 101/416...\n",
      "Processing document 111/416...\n",
      "Processing document 121/416...\n",
      "Processing document 131/416...\n",
      "Processing document 141/416...\n",
      "Processing document 151/416...\n",
      "Processing document 161/416...\n",
      "Processing document 171/416...\n",
      "Processing document 181/416...\n",
      "Processing document 191/416...\n",
      "Processing document 201/416...\n",
      "Processing document 211/416...\n",
      "Processing document 221/416...\n",
      "Processing document 231/416...\n",
      "Processing document 241/416...\n",
      "Processing document 251/416...\n",
      "Processing document 261/416...\n",
      "Processing document 271/416...\n",
      "Processing document 281/416...\n",
      "Processing document 291/416...\n",
      "Processing document 301/416...\n",
      "Processing document 311/416...\n",
      "Processing document 321/416...\n",
      "Processing document 331/416...\n",
      "Processing document 341/416...\n",
      "Processing document 351/416...\n",
      "Processing document 361/416...\n",
      "Processing document 371/416...\n",
      "Processing document 381/416...\n",
      "Processing document 391/416...\n",
      "Processing document 401/416...\n",
      "Processing document 411/416...\n",
      "Keyword extraction complete!\n",
      "\n",
      "Sample of extracted keywords:\n",
      "\n",
      "Document 1 (Tier 1):\n",
      "Celex: 22023D2270\n",
      "Title: Decision of the EEA Joint Committee No 127/2023 of...\n",
      "Keywords (14): eu 2021, 11 2023, 2023of 28, 2021 amending, april 2021...\n",
      "\n",
      "Document 2 (Tier 1):\n",
      "Celex: 22023D2272\n",
      "Title: Decision of the EEA Joint Committee No 129/2023 of...\n",
      "Keywords (14): 2023 28, 11 2023, 2023decision, agreement 2023, dec 2023...\n",
      "\n",
      "Document 3 (Tier 1):\n",
      "Celex: 22023D2277\n",
      "Title: Decision of the EEA Joint Committee No 134/2023 of...\n",
      "Keywords (14): 134 2023of, 2021 oj, 2023of 28, 2023decision, 2021 amending...\n"
     ]
    }
   ],
   "source": [
    "# Import the DynamicKeywordExtractor class from the project\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the summarization source directory to path\n",
    "summarization_src = Path('/Users/alexanderbenady/DataThesis/eu-legal-recommender/summarization/src')\n",
    "if str(summarization_src) not in sys.path:\n",
    "    sys.path.append(str(summarization_src))\n",
    "\n",
    "# Import the keyword extractor\n",
    "from keyword_extractor import DynamicKeywordExtractor\n",
    "\n",
    "# Create an instance of the keyword extractor\n",
    "keyword_extractor = DynamicKeywordExtractor()\n",
    "\n",
    "# Function to extract keywords for all documents in the DataFrame, without tqdm\n",
    "def add_extracted_keywords(df):\n",
    "    \"\"\"Extract keywords using the project's extractor and add them to the DataFrame.\"\"\"\n",
    "    # Create columns for keywords and scores\n",
    "    df['extracted_keywords'] = None\n",
    "    df['keyword_scores'] = None\n",
    "    \n",
    "    # Process each document\n",
    "    print(f\"Extracting keywords for {len(df)} documents...\")\n",
    "    for i, (idx, row) in enumerate(df.iterrows()):\n",
    "        # Print progress every 10 documents\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processing document {i+1}/{len(df)}...\")\n",
    "            \n",
    "        content = row['content']\n",
    "        if isinstance(content, str) and content.strip():\n",
    "            try:\n",
    "                # Extract keywords\n",
    "                keywords = keyword_extractor.extract_keywords(content)\n",
    "                \n",
    "                # Split keywords and scores\n",
    "                keyword_terms = [k for k, _ in keywords]\n",
    "                keyword_scores = [s for _, s in keywords]\n",
    "                \n",
    "                # Store in DataFrame\n",
    "                df.at[idx, 'extracted_keywords'] = keyword_terms\n",
    "                df.at[idx, 'keyword_scores'] = keyword_scores\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting keywords for document {row['celex_number']}: {e}\")\n",
    "                df.at[idx, 'extracted_keywords'] = []\n",
    "                df.at[idx, 'keyword_scores'] = []\n",
    "        else:\n",
    "            df.at[idx, 'extracted_keywords'] = []\n",
    "            df.at[idx, 'keyword_scores'] = []\n",
    "    \n",
    "    print(\"Keyword extraction complete!\")\n",
    "    return df\n",
    "\n",
    "# Run this function on your combined DataFrame\n",
    "sampled_docs = add_extracted_keywords(sampled_docs)\n",
    "\n",
    "# Display a few examples to verify\n",
    "print(\"\\nSample of extracted keywords:\")\n",
    "for i in range(min(3, len(sampled_docs))):\n",
    "    doc = sampled_docs.iloc[i]\n",
    "    print(f\"\\nDocument {i+1} (Tier {doc['tier']}):\")\n",
    "    print(f\"Celex: {doc['celex_number']}\")\n",
    "    print(f\"Title: {doc['title'][:50]}...\")\n",
    "    if isinstance(doc['extracted_keywords'], list):\n",
    "        print(f\"Keywords ({len(doc['extracted_keywords'])}): {', '.join(doc['extracted_keywords'][:5])}...\")\n",
    "    else:\n",
    "        print(\"No keywords extracted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>celex_number</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>summary</th>\n",
       "      <th>tier</th>\n",
       "      <th>extracted_keywords</th>\n",
       "      <th>keyword_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22023D2270</td>\n",
       "      <td>Decision of the EEA Joint Committee No 127/202...</td>\n",
       "      <td>Official Journalof the European UnionENSeries ...</td>\n",
       "      <td>Decision of the EEA Joint Committee No 127/202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[eu 2021, 11 2023, 2023of 28, 2021 amending, a...</td>\n",
       "      <td>[0.5504, 0.5604, 0.5605, 0.5646, 0.5712, 0.574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22023D2272</td>\n",
       "      <td>Decision of the EEA Joint Committee No 129/202...</td>\n",
       "      <td>Official Journalof the European UnionENSeries ...</td>\n",
       "      <td>Annex III (Transport) to the EEA Agreement — T...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2023 28, 11 2023, 2023decision, agreement 202...</td>\n",
       "      <td>[0.5764, 0.5841, 0.6078, 0.612, 0.6175, 0.619,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22023D2277</td>\n",
       "      <td>Decision of the EEA Joint Committee No 134/202...</td>\n",
       "      <td>Official Journalof the European UnionENSeries ...</td>\n",
       "      <td>Decision of the EEA Joint Committee No 134/202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[134 2023of, 2021 oj, 2023of 28, 2023decision,...</td>\n",
       "      <td>[0.5699, 0.58, 0.5876, 0.5941, 0.5945, 0.5955,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22023D2278</td>\n",
       "      <td>Decision of the EEA Joint Committee No 135/202...</td>\n",
       "      <td>Official Journalof the European UnionENSeries ...</td>\n",
       "      <td>Decision of the EEA Joint Committee No 135/202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2023 2294, 2021 oj, 2023of 28, 2021 amending,...</td>\n",
       "      <td>[0.5495, 0.556, 0.5582, 0.5735, 0.5747, 0.5756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22023D2285</td>\n",
       "      <td>Decision of the EEA Joint Committee No 142/202...</td>\n",
       "      <td>Official Journalof the European UnionENSeries ...</td>\n",
       "      <td>Decision of the EEA Joint Committee No 142/202...</td>\n",
       "      <td>1</td>\n",
       "      <td>[2023 28, 2023of 28, 105 2022, dec 2023, 2023d...</td>\n",
       "      <td>[0.5761, 0.5766, 0.5785, 0.5902, 0.5923, 0.608...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>32025L0050</td>\n",
       "      <td>Council Directive (EU) 2025/50 of 10 December ...</td>\n",
       "      <td>Official Journalof the European UnionENL serie...</td>\n",
       "      <td>**Revised Summary:**  \\n\\n**Council Directive ...</td>\n",
       "      <td>3</td>\n",
       "      <td>[esma 2026, 2025 5010, april 2024, 5010 2025, ...</td>\n",
       "      <td>[0.5225, 0.5297, 0.5349, 0.5351, 0.5357, 0.538...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>32025R0080</td>\n",
       "      <td>Commission Implementing Regulation (EU) 2025/8...</td>\n",
       "      <td>Official Journalof the European UnionENL serie...</td>\n",
       "      <td>Commission Implementing Regulation (EU) 2025/8...</td>\n",
       "      <td>3</td>\n",
       "      <td>[2024 amending, 2024gbmp123, january 2025amend...</td>\n",
       "      <td>[0.5281, 0.5295, 0.536, 0.5361, 0.5365, 0.5369...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>52024BP2307</td>\n",
       "      <td>Resolution (EU) 2024/2307 of the European Parl...</td>\n",
       "      <td>Official Journalof the European UnionENL serie...</td>\n",
       "      <td>The European Parliament adopted Resolution (EU...</td>\n",
       "      <td>3</td>\n",
       "      <td>[2024resolution, strategy 2023, published 2022...</td>\n",
       "      <td>[0.5964, 0.5993, 0.5997, 0.6049, 0.6106, 0.611...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>52024BP2340</td>\n",
       "      <td>Resolution (EU) 2024/2340 of the European Parl...</td>\n",
       "      <td>Official Journalof the European UnionENL serie...</td>\n",
       "      <td>Resolution (EU) 2024/2340 of the European Parl...</td>\n",
       "      <td>3</td>\n",
       "      <td>[launched 2024, sysper 2024, 2024 resolution, ...</td>\n",
       "      <td>[0.5827, 0.5864, 0.592, 0.5925, 0.5936, 0.5954...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>42025X0003</td>\n",
       "      <td>UN Regulation No 79 – Uniform provisions conce...</td>\n",
       "      <td>Official Journalof the European UnionENL serie...</td>\n",
       "      <td>UN Regulation No. 79 establishes uniform provi...</td>\n",
       "      <td>4</td>\n",
       "      <td>[29 2023, january 2025, approved regulations, ...</td>\n",
       "      <td>[0.5091, 0.5119, 0.5126, 0.5134, 0.5225, 0.523...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>416 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    celex_number                                              title  \\\n",
       "0     22023D2270  Decision of the EEA Joint Committee No 127/202...   \n",
       "1     22023D2272  Decision of the EEA Joint Committee No 129/202...   \n",
       "2     22023D2277  Decision of the EEA Joint Committee No 134/202...   \n",
       "3     22023D2278  Decision of the EEA Joint Committee No 135/202...   \n",
       "4     22023D2285  Decision of the EEA Joint Committee No 142/202...   \n",
       "..           ...                                                ...   \n",
       "411   32025L0050  Council Directive (EU) 2025/50 of 10 December ...   \n",
       "412   32025R0080  Commission Implementing Regulation (EU) 2025/8...   \n",
       "413  52024BP2307  Resolution (EU) 2024/2307 of the European Parl...   \n",
       "414  52024BP2340  Resolution (EU) 2024/2340 of the European Parl...   \n",
       "415   42025X0003  UN Regulation No 79 – Uniform provisions conce...   \n",
       "\n",
       "                                               content  \\\n",
       "0    Official Journalof the European UnionENSeries ...   \n",
       "1    Official Journalof the European UnionENSeries ...   \n",
       "2    Official Journalof the European UnionENSeries ...   \n",
       "3    Official Journalof the European UnionENSeries ...   \n",
       "4    Official Journalof the European UnionENSeries ...   \n",
       "..                                                 ...   \n",
       "411  Official Journalof the European UnionENL serie...   \n",
       "412  Official Journalof the European UnionENL serie...   \n",
       "413  Official Journalof the European UnionENL serie...   \n",
       "414  Official Journalof the European UnionENL serie...   \n",
       "415  Official Journalof the European UnionENL serie...   \n",
       "\n",
       "                                               summary  tier  \\\n",
       "0    Decision of the EEA Joint Committee No 127/202...     1   \n",
       "1    Annex III (Transport) to the EEA Agreement — T...     1   \n",
       "2    Decision of the EEA Joint Committee No 134/202...     1   \n",
       "3    Decision of the EEA Joint Committee No 135/202...     1   \n",
       "4    Decision of the EEA Joint Committee No 142/202...     1   \n",
       "..                                                 ...   ...   \n",
       "411  **Revised Summary:**  \\n\\n**Council Directive ...     3   \n",
       "412  Commission Implementing Regulation (EU) 2025/8...     3   \n",
       "413  The European Parliament adopted Resolution (EU...     3   \n",
       "414  Resolution (EU) 2024/2340 of the European Parl...     3   \n",
       "415  UN Regulation No. 79 establishes uniform provi...     4   \n",
       "\n",
       "                                    extracted_keywords  \\\n",
       "0    [eu 2021, 11 2023, 2023of 28, 2021 amending, a...   \n",
       "1    [2023 28, 11 2023, 2023decision, agreement 202...   \n",
       "2    [134 2023of, 2021 oj, 2023of 28, 2023decision,...   \n",
       "3    [2023 2294, 2021 oj, 2023of 28, 2021 amending,...   \n",
       "4    [2023 28, 2023of 28, 105 2022, dec 2023, 2023d...   \n",
       "..                                                 ...   \n",
       "411  [esma 2026, 2025 5010, april 2024, 5010 2025, ...   \n",
       "412  [2024 amending, 2024gbmp123, january 2025amend...   \n",
       "413  [2024resolution, strategy 2023, published 2022...   \n",
       "414  [launched 2024, sysper 2024, 2024 resolution, ...   \n",
       "415  [29 2023, january 2025, approved regulations, ...   \n",
       "\n",
       "                                        keyword_scores  \n",
       "0    [0.5504, 0.5604, 0.5605, 0.5646, 0.5712, 0.574...  \n",
       "1    [0.5764, 0.5841, 0.6078, 0.612, 0.6175, 0.619,...  \n",
       "2    [0.5699, 0.58, 0.5876, 0.5941, 0.5945, 0.5955,...  \n",
       "3    [0.5495, 0.556, 0.5582, 0.5735, 0.5747, 0.5756...  \n",
       "4    [0.5761, 0.5766, 0.5785, 0.5902, 0.5923, 0.608...  \n",
       "..                                                 ...  \n",
       "411  [0.5225, 0.5297, 0.5349, 0.5351, 0.5357, 0.538...  \n",
       "412  [0.5281, 0.5295, 0.536, 0.5361, 0.5365, 0.5369...  \n",
       "413  [0.5964, 0.5993, 0.5997, 0.6049, 0.6106, 0.611...  \n",
       "414  [0.5827, 0.5864, 0.592, 0.5925, 0.5936, 0.5954...  \n",
       "415  [0.5091, 0.5119, 0.5126, 0.5134, 0.5225, 0.523...  \n",
       "\n",
       "[416 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text by removing special characters and extra whitespace\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = text.strip()  # Remove leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenize text into words, removing stopwords\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.isalnum() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "def parse_keywords(keywords_str):\n",
    "    \"\"\"Parse keywords from the comma-separated string\"\"\"\n",
    "    if not isinstance(keywords_str, str):\n",
    "        return []\n",
    "    keywords = [k.strip() for k in keywords_str.split(',')]\n",
    "    # Split multi-word keywords into individual words\n",
    "    all_keyword_words = []\n",
    "    for kw in keywords:\n",
    "        all_keyword_words.extend(tokenize_text(kw))\n",
    "    return list(set(all_keyword_words))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compression Ratio\n",
    "def calculate_compression_ratio(original_text, summary):\n",
    "    \"\"\"Calculate compression ratio (summary length / original text length)\"\"\"\n",
    "    if not isinstance(original_text, str) or not isinstance(summary, str):\n",
    "        return np.nan\n",
    "    if len(original_text) == 0:\n",
    "        return np.nan\n",
    "    return len(summary) / len(original_text)\n",
    "\n",
    "# 2. ROUGE Scores\n",
    "rouge = Rouge()\n",
    "def calculate_rouge_scores(original_text, summary):\n",
    "    \"\"\"Calculate ROUGE-1, ROUGE-2, and ROUGE-L scores\"\"\"\n",
    "    if not isinstance(original_text, str) or not isinstance(summary, str):\n",
    "        return {'rouge-1': {'f': 0}, 'rouge-2': {'f': 0}, 'rouge-l': {'f': 0}}\n",
    "    \n",
    "    # Ensure texts are not empty and have at least one sentence\n",
    "    if len(summary.strip()) == 0 or len(original_text.strip()) == 0:\n",
    "        return {'rouge-1': {'f': 0}, 'rouge-2': {'f': 0}, 'rouge-l': {'f': 0}}\n",
    "    \n",
    "    # Handle very long texts by using the first 10k characters\n",
    "    # (Rouge can be memory-intensive with very long texts)\n",
    "    original_text = original_text[:10000]\n",
    "    \n",
    "    try:\n",
    "        scores = rouge.get_scores(summary, original_text)[0]\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating ROUGE: {e}\")\n",
    "        return {'rouge-1': {'f': 0}, 'rouge-2': {'f': 0}, 'rouge-l': {'f': 0}}\n",
    "\n",
    "# 3. Keyword Retention\n",
    "def calculate_keyword_retention(summary, keywords):\n",
    "    \"\"\"Calculate keyword retention (% of keywords present in summary)\"\"\"\n",
    "    if not isinstance(summary, str) or not keywords:\n",
    "        return 0\n",
    "    \n",
    "    # Tokenize summary\n",
    "    summary_tokens = set(tokenize_text(summary))\n",
    "    \n",
    "    # Count how many keywords appear in the summary\n",
    "    keyword_tokens = set(keywords)\n",
    "    if not keyword_tokens:\n",
    "        return 0\n",
    "    \n",
    "    found_keywords = keyword_tokens.intersection(summary_tokens)\n",
    "    retention_rate = len(found_keywords) / len(keyword_tokens) if len(keyword_tokens) > 0 else 0\n",
    "    \n",
    "    return retention_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to the sampled documents\n",
    "sampled_docs['clean_text'] = sampled_docs['content'].apply(clean_text)\n",
    "sampled_docs['clean_summary'] = sampled_docs['summary'].apply(clean_text)\n",
    "sampled_docs['parsed_keywords'] = sampled_docs['extracted_keywords'].apply(parse_keywords)\n",
    "\n",
    "# Calculate metrics for each document\n",
    "results = []\n",
    "\n",
    "for _, doc in sampled_docs.iterrows():\n",
    "    # Calculate compression ratio\n",
    "    compression_ratio = calculate_compression_ratio(doc['clean_text'], doc['clean_summary'])\n",
    "    \n",
    "    # Calculate ROUGE scores\n",
    "    rouge_scores = calculate_rouge_scores(doc['clean_text'], doc['clean_summary'])\n",
    "    \n",
    "    # Calculate keyword retention\n",
    "    keyword_retention = calculate_keyword_retention(doc['clean_summary'], doc['parsed_keywords'])\n",
    "    \n",
    "    # Store all metrics\n",
    "    result = {\n",
    "        'celex_number': doc['celex_number'],\n",
    "        'tier': doc['tier'],\n",
    "        'compression_ratio': compression_ratio,\n",
    "        'rouge_1': rouge_scores['rouge-1']['f'],\n",
    "        'rouge_2': rouge_scores['rouge-2']['f'],\n",
    "        'rouge_l': rouge_scores['rouge-l']['f'],\n",
    "        'keyword_retention': keyword_retention,\n",
    "        'original_length': len(doc['clean_text']) if isinstance(doc['clean_text'], str) else 0,\n",
    "        'summary_length': len(doc['clean_summary']) if isinstance(doc['clean_summary'], str) else 0,\n",
    "        'keyword_count': len(doc['parsed_keywords']) if isinstance(doc['parsed_keywords'], list) else 0\n",
    "    }\n",
    "    \n",
    "    results.append(result)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "metrics_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Now, let's analyze the metrics by tier and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compression_ratio</th>\n",
       "      <th>rouge_1</th>\n",
       "      <th>rouge_2</th>\n",
       "      <th>rouge_l</th>\n",
       "      <th>keyword_retention</th>\n",
       "      <th>original_length</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>keyword_count</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tier</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.061725</td>\n",
       "      <td>0.261373</td>\n",
       "      <td>0.137614</td>\n",
       "      <td>0.242573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.653212e+04</td>\n",
       "      <td>1128.302857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.285468</td>\n",
       "      <td>0.257003</td>\n",
       "      <td>0.126342</td>\n",
       "      <td>0.237620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.088276e+05</td>\n",
       "      <td>1474.507246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.006850</td>\n",
       "      <td>0.235471</td>\n",
       "      <td>0.089414</td>\n",
       "      <td>0.215125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.261365e+05</td>\n",
       "      <td>1802.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.119534</td>\n",
       "      <td>0.035901</td>\n",
       "      <td>0.104956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.463194e+06</td>\n",
       "      <td>1030.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      compression_ratio   rouge_1   rouge_2   rouge_l  keyword_retention  \\\n",
       "tier                                                                       \n",
       "1              0.061725  0.261373  0.137614  0.242573                0.0   \n",
       "2              0.285468  0.257003  0.126342  0.237620                0.0   \n",
       "3              0.006850  0.235471  0.089414  0.215125                0.0   \n",
       "4              0.000704  0.119534  0.035901  0.104956                0.0   \n",
       "\n",
       "      original_length  summary_length  keyword_count  count  \n",
       "tier                                                         \n",
       "1        2.653212e+04     1128.302857            0.0    175  \n",
       "2        1.088276e+05     1474.507246            0.0    138  \n",
       "3        8.261365e+05     1802.333333            0.0    102  \n",
       "4        1.463194e+06     1030.000000            0.0      1  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by tier and calculate mean metrics\n",
    "tier_metrics = metrics_df.groupby('tier').agg({\n",
    "    'compression_ratio': 'mean',\n",
    "    'rouge_1': 'mean',\n",
    "    'rouge_2': 'mean',\n",
    "    'rouge_l': 'mean',\n",
    "    'keyword_retention': 'mean',\n",
    "    'original_length': 'mean',\n",
    "    'summary_length': 'mean',\n",
    "    'keyword_count': 'mean',\n",
    "    'celex_number': 'count'  # Count of documents in each tier\n",
    "}).rename(columns={'celex_number': 'count'})\n",
    "\n",
    "tier_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics by Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze document length by tier\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='tier', y='original_length', data=metrics_df)\n",
    "plt.title('Original Document Length by Tier')\n",
    "plt.xlabel('Document Tier')\n",
    "plt.ylabel('Document Length (characters)')\n",
    "plt.yscale('log')  # Log scale for better visualization\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='tier', y='summary_length', data=metrics_df)\n",
    "plt.title('Summary Length by Tier')\n",
    "plt.xlabel('Document Tier')\n",
    "plt.ylabel('Summary Length (characters)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
